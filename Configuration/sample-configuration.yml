data-pulse:
  execution: 
    trigger:
      # Only one option should be provided
      # I Present here both for demonstration purposes

      # Option 1
      type: on-demand
      execute-at: #Optional - if missing run immediately
        format: hh:mm:ss.SSS-dd/MM/yyyy
        time: 20:00:00.000-18/01/2026

      # Option 2
      type: interval 
      schedule: "0 0 * * * ?" #Mandatory - Cron expression - for example: every day at midnight
    

    # Only one strategy should be provided
    # I Present here both for demonstration purposes

    # Option 1
    strategy:
      type: batch
      limitations:
        time-range: #mandatory
            format: hh:mm:ss.SSS-dd/MM/yyyy
            segmentation: none/minutes/hours/days/monthes  #options: minutes, hours, days, months, none
            gte: 13:00:00.000-15/01/2026
            lte: 15:00:00.000-15/01/2026
        entities-count: #optional
          order-by-field: INSERTION_TIME 
          value: 50
          relation: LAST #FIRST/LAST
        size: #optional - further discussion is required on this since this option will filter after the query execution - what do we do in that case?
          value: 1500MB 
    
    # Option 2
    strategy:
      type: stream
      #The following should be within the stream datasource. Here just for demonstration purposes what one of the options could be
      # context-fields: #Optional - if provided these fields values will be stored within execution result as well - kind of a metadata but not actually a metadata
      #   event_time: /path/to/event_time
      #   provider: /path/to/provider
      limitations: 
        duration: PT15M #mandatory - for example: PT15M means (15 minutes)
        entities-count: 10 #optional - number of records to be processed in each execution. relation isn't needed in this case since it's always LAST
        size: #optional - further discussion is required on this since this option will filter after the query execution - what do we do in that case?
          value: 1500MB 
    

  datasources:
    raw: #some name - would be helpful for linking which validations for each source if it gets complicated we could deploy additional instance
      type: oracle
      jdbc-url: jdbc:oracle:thin:@//localhost:1521/XEPDB1
      username: user
      password: password
      source:
        primary-table: 
          name: SOURCE_TABLE
          alias: s
        joins: #Optional - if missing no-joins will be applied
          - type: left
            name: "JOINED_TABLE_1"
            alias: jt1
            on-clause: "RAW.FIELD_A = JOINED_TABLE_1.FIELD_A"

          - type: inner
            name: "JOINED_TABLE_2"
            alias: jt2
            on-clause: "RAW.FIELD_B = JOINED_TABLE_2.FIELD_B"
      where-clause: | #Optional - can be used to filter our data sample even more
        FIELD1 = 'VALUE1' AND IS_ON_SALE = TRUE AND......
      
      limitations: # Optional - if provided in both places then use these values instead of the project level ones
        time-range: #mandatory - if isn't provided at project level
            format: hh:mm:ss.SSS-dd/MM/yyyy
            segmentation: none/minutes/hours/days/monthes  #options: minutes, hours, days, months, none
            gte: 14:00:00.000-18/01/2026
            lte: 16:00:00.000-18/01/2026
        rows-count: #optional
          order-by-field: INSERTION_TIME 
          value: 50
          relation: LAST #FIRST/LAST
        size: #optional - further discussion is required on this since this option will filter after the query execution - what do we do in that case?
          value: 1500MB 

    yellow_taxi_2019_01:
      type: oracle
      jdbc-url: jdbc:oracle:thin:@//localhost:1521/XEPDB1
      username: user
      password: password
      table: yellow_taxi_rides

  dq-tests: 
    - name: "Missing Pickup Time"
      description: "Checks the rides count with missing pickup time"
      datasource: yellow_taxi_2019_01
      type: sql
      sink: yellow_taxi_sink
      query: | #{data-sample} means a predefined keyword that we will require the users in order to inject the data-sample query
        SELECT
          SUM(CASE WHEN pickup_datetime IS NULL THEN 1 ELSE 0 END) AS null_pickup_dt,
        FROM {data-sample};
      result: #Optional - only if we want to label the execution result with successful/unsuccessful indicator (different options for metric or values list)
        type: metric
        metric-field: null_pickup_dt
        thresholds: 
          sanity-threshold: 
            value: 30
            relation: LTE #Another option could be LT/GT/GTE
          acceptable-threshold:   
            value: 50
            relation: LTE

    - name: "Payment type distinct values"
      description: "Chekcs the payment types distinct values"
      datasource: yellow_taxi_2019_01
      type: sql
      sink: yellow_taxi_sink
      query: |
        SELECT DISTINCT payment_type
        FROM {data-sample}
        ORDER BY payment_type;
      result:
        type: values-list
        expected-values:
          - 'Credit Card'
          - 'Cash'
          - 'Bitcoin'

    - name: "Payment type segmentation"
      description: "Chekcs the payment type segmentation"
      datasource: yellow_taxi_2019_01
      type: sql
      sink: yellow_taxi_sink
      query: |
        SELECT
          payment_type,
          COUNT(*) AS rides_count
        FROM {data-sample}
        GROUP BY payment_type;
      result:
        metric-field: rides_count
        sanity-threshold: 
          value: 30
          relation: LTE
        acceptable-threshold:   
          value: 50
          relation: LTE

    - name: "Ride duration percentiles"
      description: "Checks the rides duration percentiles"
      datasource: yellow_taxi_2019_01
      type: sql
      sink: yellow_taxi_sink
      query: |
        SELECT
          PERCENTILE_CONT(0.50) WITHIN GROUP (ORDER BY ride_duration_min) AS p50_minutes,
          PERCENTILE_CONT(0.90) WITHIN GROUP (ORDER BY ride_duration_min) AS p90_minutes,
        FROM (
          SELECT
            (tpep_dropoff_datetime - tpep_pickup_datetime) * 24 * 60 AS ride_duration_min
          FROM {data-sample}
        )

  output-sinks:
    yellow_taxi_sink:
      type: elasticsearch
      host: http://localhost:9200
      index: dq-output-index

    raw-metrics:
      type: elasticsearch
      host: http://localhost:9200
      index: dq-output-index

    processed-metrics:
      type: filesystem
      target-path: /some/path
